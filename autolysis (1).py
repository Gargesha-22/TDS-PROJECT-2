# -*- coding: utf-8 -*-
"""autolysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ouGL_F94txC4H5r7_-2WyJiRHXhmVR1l
"""

# /// script
# requires-python = ">=3.9"
# dependencies = [
#   "pandas",
#   "seaborn",
#   "matplotlib",
#   "numpy",
#   "scipy",
#   "openai",
#   "scikit-learn",
#   "requests",
#   "ipykernel",  # Added ipykernel
# ]
# ///

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import argparse
import requests
import json
import openai
import chardet

# Function to read the CSV file with proper encoding handling
def read_data_with_fallback_encoding(file_path):
    try:
        # First try with ISO-8859-1 encoding
        df = pd.read_csv(file_path, encoding='ISO-8859-1')
        print("File read successfully with ISO-8859-1 encoding.")
        return df
    except Exception as e:
        print(f"Error reading the file: {e}")
        return None

# Data Analysis Function
def analyze_data(df):
    print("ðŸ“ Basic Dataset Information:")
    print(f"Shape: {df.shape}")
    print(f"Columns: {list(df.columns)}\n")
    print(df.info())

    # 2. Summary Statistics for Numeric Columns
    print("\nðŸ“Š Summary Statistics (Numeric Columns):")
    numeric_df = df.select_dtypes(include=[np.number])  # Only numeric columns
    print(numeric_df.describe())

    # 3. Missing Values
    print("\nðŸ” Missing Values per Column:")
    missing_values = df.isnull().sum()
    print(missing_values[missing_values > 0])

    # 4. Correlation Matrix for Numeric Columns
    print("\nðŸ“ˆ Correlation Matrix (Numeric Columns):")
    correlation_matrix = numeric_df.corr()  # Only numeric columns
    print(correlation_matrix)

    # Visualize Correlation Matrix
    plt.figure(figsize=(10, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
    plt.title("Correlation Matrix")
    plt.savefig("correlation_matrix.png")
    plt.close()

    # 5. Outlier Detection (IQR Method)
    print("\nðŸš¨ Outlier Detection (Numeric Columns):")
    def detect_outliers(col):
        Q1 = col.quantile(0.25)
        Q3 = col.quantile(0.75)
        IQR = Q3 - Q1
        outliers = col[(col < Q1 - 1.5 * IQR) | (col > Q3 + 1.5 * IQR)]
        return len(outliers)

    for col in numeric_df:
        num_outliers = detect_outliers(numeric_df[col])
        print(f"{col}: {num_outliers} outliers detected")

    # 6. Handle Categorical Columns
    print("\nðŸ—‚ï¸ Categorical Data Insights:")
    categorical_df = df.select_dtypes(include=['object', 'category'])
    if not categorical_df.empty:
        print("Categorical Columns Summary:")
        print(categorical_df.describe())
        print("\nFrequency of each category:")
        for col in categorical_df:
            print(f"--- {col} ---")
            print(categorical_df[col].value_counts())

    # 7. Clustering Suggestion
    print("\nðŸ”— Clustering Suggestion:")
    if len(df) > 1000:
        print("Dataset is large. Consider using clustering techniques like K-Means.")
    else:
        print("Dataset size is small. Clustering might not be required.")

    # 8. Hierarchy Detection
    print("\nðŸ“‚ Basic Hierarchy Detection (Relationships between Columns):")
    if len(categorical_df.columns) >= 2:
        print(f"Columns '{categorical_df.columns[0]}' and '{categorical_df.columns[1]}' might form hierarchical relationships.")
    else:
        print("Not enough categorical columns to detect hierarchy.")

    print("\nâœ… Data Analysis Completed.\n")

# Function to generate the visualizations (one of each type)
def generate_visualizations(df):
    # 1. Missing Values Visualization
    missing_values = df.isnull().sum()
    missing_values = missing_values[missing_values > 0]
    if not missing_values.empty:
        plt.figure(figsize=(8, 6))
        missing_values.plot(kind="bar", color="red")
        plt.title("Missing Values per Column")
        plt.xlabel("Columns")
        plt.ylabel("Count of Missing Values")
        plt.savefig("missing_values.png")
        plt.close()
        print("âœ… Missing values visualization saved.")
    else:
        print("No missing values found. Skipping visualization.")

    # 2. Correlation Matrix Heatmap
    numeric_cols = df.select_dtypes(include=["number"])
    if not numeric_cols.empty:
        plt.figure(figsize=(10, 8))
        correlation_matrix = numeric_cols.corr()
        sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
        plt.title("Correlation Matrix Heatmap")
        plt.savefig("correlation_heatmap.png")
        plt.close()
        print("âœ… Correlation matrix heatmap saved.")

    # 3. Boxplot for the first numeric column
    if not numeric_cols.empty:
        col = numeric_cols.columns[0]  # Select the first numeric column
        plt.figure(figsize=(6, 4))
        sns.boxplot(x=df[col], color="blue")
        plt.title(f"Boxplot for {col}")
        plt.savefig(f"boxplot_{col}.png")
        plt.close()
        print(f"âœ… Boxplot for {col} saved.")

    # 4. Histogram for the first numeric column
    if not numeric_cols.empty:
        col = numeric_cols.columns[0]  # Select the first numeric column
        plt.figure(figsize=(6, 4))
        df[col].hist(bins=30, color="green")
        plt.title(f"Histogram of {col}")
        plt.xlabel(col)
        plt.ylabel("Frequency")
        plt.savefig(f"histogram_{col}.png")
        plt.close()
        print(f"âœ… Histogram for {col} saved.")

    # 5. Bar Chart for the first categorical column (if any)
    category_cols = df.select_dtypes(include=["object", "category"]).columns
    if category_cols.size > 0:
        col = category_cols[0]  # Select the first categorical column
        plt.figure(figsize=(8, 6))
        df[col].value_counts().plot(kind="bar", color="orange")
        plt.title(f"Bar Chart for {col}")
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.savefig(f"barchart_{col}.png")
        plt.close()
        print(f"âœ… Bar chart for {col} saved.")

    print("\nðŸŽ‰ All visualizations have been saved in the current directory.")

# Example usage
if __name__ == "__main__":
    # Example file path (replace with your actual file path)
    file_path = "/content/happiness.csv"  # Replace with your actual file path

    # Read data
    df = pd.read_csv(file_path, encoding="ISO-8859-1")

    # Generate the visualizations
    generate_visualizations(df)

# Function to create the README.md file with a narrative and visualizations
def create_readme(df, output_dir=".", images=None):
    # Check if images were provided
    if images is None:
        images = []

    # Open the README file in write mode
    readme_path = os.path.join(output_dir, "README.md")
    with open(readme_path, "w") as f:
        # Header
        f.write("# Automated Data Analysis Report\n\n")

        # Overview Section
        f.write("## Overview\n")
        f.write("This script performs an automated analysis of the given dataset. It includes basic "
                "information, summary statistics, missing values detection, correlation matrix, "
                "outlier detection, clustering suggestions, and hierarchy detection for categorical columns.\n\n")

        # Dataset Information Section
        f.write("## Dataset Information\n")
        f.write(f"- Shape: {df.shape}\n")
        f.write(f"- Columns: {', '.join(df.columns)}\n\n")
        f.write("### Data Types:\n")
        f.write(f"{df.dtypes}\n\n")

        # Summary Statistics Section
        f.write("## Summary Statistics\n")
        f.write("### Basic Statistical Summary:\n")
        f.write(f"{df.describe()}\n\n")

        # Missing Values Section
        missing_values = df.isnull().sum()
        missing_values = missing_values[missing_values > 0]
        f.write("## Missing Values\n")
        if not missing_values.empty:
            f.write(f"### Columns with Missing Values:\n")
            f.write(f"{missing_values}\n\n")
        else:
            f.write("No missing values detected.\n\n")

        # Correlation Matrix Section
        f.write("## Correlation Matrix\n")
        f.write("### Correlation Between Numerical Columns:\n")
        correlation_matrix = df.corr()
        f.write(f"{correlation_matrix}\n\n")

        # Save correlation matrix as a plot
        plt.figure(figsize=(10, 6))
        sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
        correlation_image = os.path.join(output_dir, "correlation_matrix.png")
        plt.savefig(correlation_image)
        plt.close()

        # Include the plot in the README
        f.write("![Correlation Matrix](correlation_matrix.png)\n\n")

        # Outlier Detection Section (IQR Method)
        def detect_outliers(col):
            Q1 = col.quantile(0.25)
            Q3 = col.quantile(0.75)
            IQR = Q3 - Q1
            outliers = col[(col < Q1 - 1.5 * IQR) | (col > Q3 + 1.5 * IQR)]
            return len(outliers)

        f.write("## Outlier Detection\n")
        numeric_cols = df.select_dtypes(include=[float, int])
        for col in numeric_cols:
            num_outliers = detect_outliers(df[col])
            f.write(f"- {col}: {num_outliers} outliers detected\n")
        f.write("\n")

        # Visualizations Section
        if images:
            f.write("## Visualizations\n")
            for image in images:
                f.write(f"![{image}](./{image})\n\n")

        # Clustering Suggestion
        f.write("## Clustering Suggestion\n")
        if len(df) > 1000:
            f.write("The dataset is large. Consider using clustering techniques like K-Means.\n")
        else:
            f.write("The dataset is small. Clustering may not be necessary.\n\n")

        # Hierarchy Detection Section
        f.write("## Hierarchy Detection\n")
        category_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(category_cols) >= 2:
            f.write(f"Columns `{category_cols[0]}` and `{category_cols[1]}` might form hierarchical relationships.\n")
        else:
            f.write("Not enough categorical columns to detect hierarchy.\n\n")

        f.write("## Conclusion\n")
        f.write("This automated analysis provides insights into the structure of the dataset, including "
                "summary statistics, missing data, relationships between numerical variables, and "
                "outliers. Further analysis can be performed based on the insights generated.\n")

    print(f"README.md has been created and saved to {readme_path}")

def read_data_with_fallback_encoding(file_path):
    try:
        # Attempt to read with UTF-8 encoding
        df = pd.read_csv(file_path, encoding='utf-8')
        print("File read successfully with UTF-8 encoding.")
        return df
    except UnicodeDecodeError:
        print("UTF-8 failed. Trying ISO-8859-1 encoding...")
        try:
            # Fallback to ISO-8859-1 encoding
            df = pd.read_csv(file_path, encoding='ISO-8859-1')
            print("File read successfully with ISO-8859-1 encoding.")
            return df
        except Exception as e:
            print(f"Error reading the file: {e}")
            return None

# Example usage
file_path = "/content/happiness.csv"  # Replace with your actual file path
df = read_data_with_fallback_encoding(file_path)

import openai

def generate_detailed_story(df, output_dir=".", images=None, api_token=None):
    """
    Generates a detailed story based on the dataset analysis.

    Parameters:
    - df: Pandas DataFrame with the dataset.
    - output_dir: Directory to save the README.md and images.
    - images: List of image filenames to include in the narrative (optional).
    - api_token: The AI Proxy token for OpenAI API.

    Returns:
    - The generated story as a string.
    """

    # Set the AI Proxy token
    openai.api_key = api_token or os.getenv("AIPROXY_TOKEN")

    # Prepare the summary data to send to the LLM
    dataset_info = {
        "shape": df.shape,
        "columns": list(df.columns),
        "data_types": str(df.dtypes),
        "summary_statistics": df.describe().to_dict(),
        "missing_values": df.isnull().sum().to_dict(),
        "correlation_matrix": df.corr().to_dict()  # Correlation matrix for numeric columns
    }

    # Step 2: Create the prompt for the LLM
    prompt = f"""
    Write a detailed story about the analysis of the following dataset. The story should be coherent,
    explaining the dataset, analysis steps, insights gained, and implications. Include insights about
    the dataset's structure, summary statistics, missing data, relationships between variables, outliers,
    clustering suggestions, and potential hierarchical relationships between categorical columns.

    Here is the dataset information:
    Dataset Shape: {dataset_info['shape']}
    Columns: {', '.join(dataset_info['columns'])}
    Data Types: {dataset_info['data_types']}

    Summary Statistics:
    {json.dumps(dataset_info['summary_statistics'], indent=2)}

    Missing Values per Column:
    {json.dumps(dataset_info['missing_values'], indent=2)}

    Correlation Matrix (Numerical Columns):
    {json.dumps(dataset_info['correlation_matrix'], indent=2)}

    If there are any outliers, explain their significance. If there are relationships between categorical
    columns, describe the potential hierarchical structures. Based on the dataset size, suggest whether
    clustering techniques like K-Means would be beneficial.

    You may include visualizations to support your narrative. Refer to the images (PNG files) below:
    """

    # Step 3: If there are images (visualizations), include them in the prompt
    image_references = []
    if images:
        for image in images:
            image_path = os.path.join(output_dir, image)
            image_references.append(f"![{image}]({image_path})")

    # Include the image references in the prompt
    prompt += "\n".join(image_references) + "\n"

    # Step 4: Request the detailed story from the LLM via AI Proxy (OpenAI API)
    try:
        # Request the LLM to generate the story using the new API interface
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.7,
            n=1,
            stop=["\n"]
        )

        # Extract the story from the response
        story = response['choices'][0]['message']['content'].strip()

        # Save the generated story to a file
        story_file = os.path.join(output_dir, "analysis_story.md")
        with open(story_file, "w") as f:
            f.write(story)

        print(f"Story generated successfully and saved to {story_file}")

        return story

    except Exception as e:
        print(f"Error generating story: {e}")
        return None

# Perform the analysis and generate the story
analyze_data(df)
story = generate_detailed_story(df, output_dir=".", api_token="eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIyZjEwMDE3NTFAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.Jstz7rAnOcYgOdYoRDJCZAuJltFnACH_0yBKYTNdJ0Y")

